{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fongj\\AppData\\Local\\Temp\\ipykernel_18680\\3463287791.py:2: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Processed_df.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Processed_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature engineering: polarity and subjectivity\n",
    "\n",
    "Polarity: numerical score that quantifies the sentiment of the text on a continuous scale. It measures how positive or negative the text is. Polarity scores typically range from -1 (extremely negative) to 1 (extremely positive), with 0 indicating neutral sentiment.\n",
    "\n",
    "Subjectivity: Subjectivity measures the degree to which the text is subjective or opinion-based rather than objective. Subjectivity is also represented as a numerical score ranging from 0 to 1. A score closer to 0 suggests that the text is more objective, factual, or informational. A score closer to 1 suggests that the text is more subjective and opinion-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['tokens'] = df['reviewContent'].apply(tokenizer.tokenize)\n",
    "\n",
    "# Define a function to remove stopwords and punctuation\n",
    "def preprocess_text(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens]  # Convert to lowercase\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]  # Remove punctuation\n",
    "    tokens = [word for word in tokens if len(word) > 1]  # Remove single-character words\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the 'tokens' column\n",
    "df['clean_tokens'] = df['tokens'].apply(preprocess_text)\n",
    "\n",
    "# Join the clean tokens back into sentences\n",
    "df['clean_text'] = df['clean_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Sentiment Analysis using TextBlob\n",
    "df['sentiment'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment)\n",
    "\n",
    "# Extract polarity and subjectivity scores from the sentiment analysis\n",
    "df['polarity'] = df['sentiment'].apply(lambda x: x.polarity)\n",
    "df['subjectivity'] = df['sentiment'].apply(lambda x: x.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram analysis (common phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram: go back - Frequency: 2121\n",
      "N-gram: really good - Frequency: 1303\n",
      "N-gram: first time - Frequency: 1154\n",
      "N-gram: food good - Frequency: 1143\n",
      "N-gram: great food - Frequency: 1094\n",
      "N-gram: deep dish - Frequency: 1061\n",
      "N-gram: pretty good - Frequency: 1013\n",
      "N-gram: food great - Frequency: 980\n",
      "N-gram: great place - Frequency: 941\n",
      "N-gram: love place - Frequency: 916\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "# Define the n-gram range (e.g., 2 for bigrams)\n",
    "ngram_range = 2  # For bigrams (adjust as needed)\n",
    "\n",
    "# Function to extract n-grams from a list of tokens\n",
    "def extract_ngrams(tokens, n):\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Extract n-grams and store them in a new column\n",
    "df['ngrams'] = df['clean_tokens'].apply(lambda x: extract_ngrams(x, ngram_range))\n",
    "\n",
    "# Flatten the list of n-grams\n",
    "all_ngrams = [ngram for ngram_list in df['ngrams'] for ngram in ngram_list]\n",
    "\n",
    "# Count the frequency of each n-gram\n",
    "ngram_freq = Counter(all_ngrams)\n",
    "\n",
    "# Print the most common n-grams and their frequencies\n",
    "most_common_ngrams = ngram_freq.most_common(10)\n",
    "for ngram, freq in most_common_ngrams:\n",
    "    print(f\"N-gram: {' '.join(ngram)} - Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fongj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fongj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\fongj\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fongj\\Downloads\\BT4012\\BT4012_Reviews\\Notebooks\\feature engineering\\feature_engineering.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fongj/Downloads/BT4012/BT4012_Reviews/Notebooks/feature%20engineering/feature_engineering.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fongj/Downloads/BT4012/BT4012_Reviews/Notebooks/feature%20engineering/feature_engineering.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fongj/Downloads/BT4012/BT4012_Reviews/Notebooks/feature%20engineering/feature_engineering.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, TensorDataset, random_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fongj/Downloads/BT4012/BT4012_Reviews/Notebooks/feature%20engineering/feature_engineering.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdependency_versions_table\u001b[39;00m \u001b[39mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversions\u001b[39;00m \u001b[39mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[39m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[39m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtqdm\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpyyaml\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt', max_length=512, truncation=True)\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bert_sentiment\"] = df[\"reviewContent\"].apply(lambda x: sentiment_score(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bert_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with columns 'bert_sentiment' and 'class'\n",
    "\n",
    "# Create separate histograms for fraud and non-fraud classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram for the \"bert_sentiment\" of the fraud class\n",
    "sns.histplot(data=df[df['flagged'] == \"Y\"], x='bert_sentiment', kde=True, color='red', label='Fraud')\n",
    "\n",
    "# Histogram for the \"bert_sentiment\" of the non-fraud class\n",
    "sns.histplot(data=df[df['flagged'] == 'N'], x='bert_sentiment', kde=True, color='blue', label='Non-Fraud')\n",
    "\n",
    "plt.title('Distribution of BERT Sentiment Scores')\n",
    "plt.xlabel('BERT Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Set the x-axis limits to be between 1 and 5\n",
    "plt.xlim(1, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(df[df[\"flagged\"]==\"N\"][\"bert_sentiment\"], shade=True, color=\"green\", label=\"Genuine\", ax=ax)\n",
    "\n",
    "sns.kdeplot(df[df[\"flagged\"]==\"Y\"][\"bert_sentiment\"], shade=True, color=\"blue\", label=\"Fraudulent\", ax=ax)\n",
    "ax.set_xlabel(\"Sentiment Score \")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend()\n",
    "fig.suptitle(\"Sentiment Value vs Fraud Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-]\n",
    "df_bert = pd.DataFrame({\n",
    "    'id': range(len(df)),\n",
    "    'label': df['flagged'],\n",
    "    'alpha': ['a']*df.shape[0],\n",
    "    'text': df['reviewContent'].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "# Splitting training data file into *train* and *dev*\n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    "\n",
    "df_bert_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, Trainer, TrainingArguments\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(df_bert_train['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "dev_encodings = tokenizer(df_bert_dev['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create a Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewsDataset(train_encodings, df_bert_train['label'].tolist())\n",
    "dev_dataset = ReviewsDataset(dev_encodings, df_bert_dev['label'].tolist())\n",
    "\n",
    "# Model and training setup\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./results',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
